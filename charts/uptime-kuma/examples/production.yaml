# Production deployment example for Uptime Kuma
# Includes StatefulSet, persistence, ingress with TLS, and high availability

# ==============================================================================
# Image Configuration
# ==============================================================================

image:
  repository: louislam/uptime-kuma
  tag: "2"
  pullPolicy: IfNotPresent

# ==============================================================================
# Uptime Kuma Configuration
# ==============================================================================

uptimeKuma:
  # Enable Docker socket for container monitoring
  dockerSocket:
    enabled: true
    hostPath: /var/run/docker.sock

# ==============================================================================
# Controller Configuration (StatefulSet for production)
# ==============================================================================

controller:
  type: statefulset
  replicas: 1  # Uptime Kuma uses SQLite by default, single replica recommended
  podManagementPolicy: OrderedReady
  updateStrategy:
    type: RollingUpdate

# ==============================================================================
# Service Configuration
# ==============================================================================

service:
  type: ClusterIP
  port: 3001
  annotations: {}

# ==============================================================================
# Ingress Configuration
# ==============================================================================

ingress:
  enabled: true
  className: "nginx"
  annotations:
    cert-manager.io/cluster-issuer: "letsencrypt-prod"
    nginx.ingress.kubernetes.io/proxy-body-size: "50m"
    nginx.ingress.kubernetes.io/websocket-services: "uptime-kuma"
    nginx.ingress.kubernetes.io/proxy-read-timeout: "3600"
    nginx.ingress.kubernetes.io/proxy-send-timeout: "3600"
  hosts:
    - host: uptime.example.com
      paths:
        - path: /
          pathType: Prefix
  tls:
    - secretName: uptime-kuma-tls
      hosts:
        - uptime.example.com

# ==============================================================================
# Persistence Configuration
# ==============================================================================

persistence:
  enabled: false  # Using volumeClaimTemplate for StatefulSet
  volumeClaimTemplate:
    enabled: true
    storageClassName: "fast-ssd"
    accessMode: ReadWriteOnce
    size: 10Gi

# ==============================================================================
# Security Contexts
# ==============================================================================

podSecurityContext:
  runAsNonRoot: true
  runAsUser: 1000
  runAsGroup: 1000
  fsGroup: 1000
  fsGroupChangePolicy: "OnRootMismatch"
  seccompProfile:
    type: RuntimeDefault

securityContext:
  allowPrivilegeEscalation: false
  runAsNonRoot: true
  runAsUser: 1000
  runAsGroup: 1000
  readOnlyRootFilesystem: false
  capabilities:
    drop:
      - ALL

# ==============================================================================
# Resources
# ==============================================================================

resources:
  limits:
    cpu: 2000m
    memory: 1Gi
  requests:
    cpu: 500m
    memory: 512Mi

# ==============================================================================
# Health Probes
# ==============================================================================

livenessProbe:
  enabled: true
  httpGet:
    path: /
    port: http
  initialDelaySeconds: 60
  periodSeconds: 10
  timeoutSeconds: 5
  failureThreshold: 5

readinessProbe:
  enabled: true
  httpGet:
    path: /
    port: http
  initialDelaySeconds: 30
  periodSeconds: 5
  timeoutSeconds: 3
  failureThreshold: 3

startupProbe:
  enabled: true
  httpGet:
    path: /
    port: http
  initialDelaySeconds: 0
  periodSeconds: 10
  timeoutSeconds: 5
  failureThreshold: 60

# ==============================================================================
# High Availability (optional - requires external database)
# ==============================================================================

# Note: For true HA with multiple replicas, you need to configure
# an external MySQL/MariaDB database instead of SQLite

# ==============================================================================
# Pod Disruption Budget
# ==============================================================================

pdb:
  enabled: true
  minAvailable: 1

# ==============================================================================
# Network Policy
# ==============================================================================

networkPolicy:
  enabled: true
  policyTypes:
    - Ingress
    - Egress
  ingress:
    - from:
        - namespaceSelector:
            matchLabels:
              name: ingress-nginx
      ports:
        - protocol: TCP
          port: 3001
  egress:
    # Allow all egress for monitoring external services
    - {}

# ==============================================================================
# Monitoring
# ==============================================================================

monitoring:
  serviceMonitor:
    enabled: true
    interval: 60s
    scrapeTimeout: 30s
    labels:
      prometheus: kube-prometheus

  prometheusRule:
    enabled: true
    labels:
      prometheus: kube-prometheus
    rules:
      - alert: UptimeKumaDown
        expr: up{job="uptime-kuma"} == 0
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Uptime Kuma is down"
          description: "Uptime Kuma has been down for more than 5 minutes"

      - alert: UptimeKumaHighMemory
        expr: container_memory_usage_bytes{pod=~"uptime-kuma.*"} / container_spec_memory_limit_bytes{pod=~"uptime-kuma.*"} > 0.9
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Uptime Kuma high memory usage"
          description: "Uptime Kuma is using more than 90% of its memory limit"

# ==============================================================================
# Scheduling
# ==============================================================================

affinity:
  podAntiAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          labelSelector:
            matchLabels:
              app.kubernetes.io/name: uptime-kuma
          topologyKey: kubernetes.io/hostname
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Mailrise is down"
          description: "Mailrise instance {{ $labels.instance }} has been down for more than 5 minutes"

networkPolicy:
  enabled: true
  policyTypes:
    - Ingress
    - Egress
  ingress:
    - from:
        - namespaceSelector: {}
      ports:
        - protocol: TCP
          port: 8025
  egress:
    # Allow all egress for Apprise notifications
    - {}
